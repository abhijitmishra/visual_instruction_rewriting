{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db4c0584",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\\\MiniConda3\\envs\\VLM\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph: A person eating a yellow banana in a spaceship. This man looks happy and satasfying and am trying to sit on the chair, but due lack of gravity, he could not.\n",
      "Query: Base on the paragraph, What is the man doing and where is he at?\n",
      "Replyer: The pilot is looking at an area where the cabin and the other rooms of the spaceship are connected. The ship is in the middle of the highway, and no pilot and the space station are visible. The pilot is seated at the seat of one of the spaceships, and the space station is near to the left and center of the space station. There are two astronauts waiting for the space station in front of the left and right spaceships.\n",
      "Question: Is this man sleeping in the other spaceships, or in another of the ships?\n",
      "Answer: This man is waiting at an altitude of 100 kilometers and is seated in the center of a space station. He has a\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Load the pre-trained GPT-3.5 model and tokenizer\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "def generate_response(paragraph, query):\n",
    "    # Concatenate the paragraph and query\n",
    "    input_text = f\"Paragraph: {paragraph}\\nQuery: {query}\"\n",
    "    \n",
    "    # Encode the input text\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "    \n",
    "    # Generate the response\n",
    "    output = model.generate(input_ids, max_length=200, num_return_sequences=1, do_sample=True, top_k=50, top_p=0.95, num_beams=1)\n",
    "    \n",
    "    # Decode the output\n",
    "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Example usage\n",
    "paragraph = \"A person eating a yellow banana in a spaceship. This man looks happy and satasfying and am trying to sit on the chair, but due lack of gravity, he could not.\"\n",
    "query = \"Base on the paragraph, What is the man doing and where is he at?\"\n",
    "response = generate_response(paragraph, query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea300e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: {\n",
      "    \"error\": {\n",
      "        \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.\",\n",
      "        \"type\": \"insufficient_quota\",\n",
      "        \"param\": null,\n",
      "        \"code\": \"insufficient_quota\"\n",
      "    }\n",
      "}\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "def generate_response(paragraph, query):\n",
    "    # Concatenate the paragraph and query\n",
    "    input_text = f\"Paragraph: {paragraph}\\nQuery: {query}\"\n",
    "\n",
    "    # Set up the request parameters\n",
    "    data = {\n",
    "        \"prompt\": input_text,\n",
    "        \"max_tokens\": 200,\n",
    "        \"n\": 1,\n",
    "        \"stop\": None,\n",
    "        \"temperature\": 0.7,\n",
    "        \"model\": \"gpt-3.5-turbo-0125\"\n",
    "    }\n",
    "\n",
    "    # Set up the OpenAI API endpoint and headers\n",
    "    endpoint = \"https://api.openai.com/v1/completions\"\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": \"Bearer sk-proj-\"\n",
    "    }\n",
    "\n",
    "    # Make the request to the API\n",
    "    response = requests.post(endpoint, json=data, headers=headers)\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Extract the generated text\n",
    "        generated_text = response.json()[\"choices\"][0][\"text\"].strip()\n",
    "        return generated_text\n",
    "    else:\n",
    "        # Handle any errors\n",
    "        print(f\"Error: {response.text}\")\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "paragraph = \"Extract the intent and arguments by performing semantic parsing of the following sentence. Give the output in JSON format:\"\n",
    "query = \"Base on the paragraph, What is the man doing and where is he at?\"\n",
    "response = generate_response(paragraph, query)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78f1310a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given paragraph: A person eating a yellow banana in a spaceship. This man looks happy and satasfying and am trying to sit on the chair, but due lack of gravity, he could not.\n",
      "Base on the paragraph, What is the man eating?\n",
      "\n",
      "The man in the illustration is eating a yellow banana, and he is sitting on a chair in a spaceship. He is wearing a spacesuit and has a spaceship on his back. He is looking happy and sitting on the chair, which could be interpreted as a man feeling content and fulfilled in his life.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM\n",
    "torch.cuda.empty_cache()\n",
    "# Specify the model path\n",
    "model_path = 'mtgv/MobileLLaMA-1.4B-Chat'\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# Set the device to CUDA\n",
    "device = torch.device('cuda')\n",
    "\n",
    "try:\n",
    "    # Load the model with half precision for reduced memory usage and move it to the GPU\n",
    "    model = LlamaForCausalLM.from_pretrained(model_path, torch_dtype=torch.float16).to(device)\n",
    "except RuntimeError as e:\n",
    "    if 'out of memory' in str(e):\n",
    "        print('OOM when loading the model. Try reducing the batch size or image resolution.')\n",
    "    raise e\n",
    "\n",
    "def generate_response(paragraph, query):\n",
    "    # Concatenate the paragraph and query\n",
    "    input_text = f\"Given paragraph: {paragraph}\\nBase on the paragraph, {query}\"\n",
    "\n",
    "    # Encode the input text\n",
    "    input_ids = tokenizer(input_text, return_tensors='pt').input_ids.to(device)\n",
    "\n",
    "    # Generate the response\n",
    "    output_ids = model.generate(input_ids, max_length=200, num_return_sequences=1, do_sample=True, top_k=50, top_p=0.95, num_beams=1)[0]\n",
    "\n",
    "    # Decode the output\n",
    "    response = tokenizer.decode(output_ids, skip_special_tokens=True)\n",
    "\n",
    "    return response\n",
    "\n",
    "# Example usage\n",
    "paragraph = \"A person eating a yellow banana in a spaceship. This man looks happy and satasfying and am trying to sit on the chair, but due lack of gravity, he could not.\"\n",
    "query = \"What is the man eating?\"\n",
    "response = generate_response(paragraph, query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da79cff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VLM",
   "language": "python",
   "name": "vlm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
