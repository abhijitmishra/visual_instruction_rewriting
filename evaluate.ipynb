{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import ReVisionProcessor, ReVisionForConditionalGeneration\n",
    "\n",
    "from datautils import RevisionRewriteDataset\n",
    "# from datautils import RevisionRewriteDatasetWithMetadata\n",
    "from transformers import BitsAndBytesConfig\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "import evaluate\n",
    "import nltk\n",
    "\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"omw-1.4\")\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(\".env\")\n",
    "\n",
    "from prettytable import PrettyTable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model/Data Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the evaluation parameters/variables.\n",
    "\n",
    "- `DEVICE` determines the compute device the tensors are sent to.\n",
    "- `MODEL_ID` is the repository name for the model that you are evaluating\n",
    "- `HF_TOKEN` obtains the HuggingFace authentication token from the environment. If you are getting an error on this line, either add it to the environment or create a `.env` file with `HF_TOKEN`. See the imports for `load_dotenv`.\n",
    "- `BATCH_SIZE` is the number of examples run at a time. Higher number for speed, but setting it to 1 is the most accurate (used for the paper)\n",
    "- `DATASET_SUFFIX` is the suffix in the dataset file name on our HuggingFace repo.\n",
    "- `save_location` is the local file location that the paired evaluated completions and their corresponding reference text are stored.\n",
    "\n",
    "Note that you may or may not get an error with the `images` folder. You may need to go into the Dataset object defined in `datautils.py` and modify the images folder. Not sure why it's sometimes `images`, `images/images`, or `images/images/images`..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\"\n",
    "MODEL_ID = \"anonymoususerrevision/ReVision-250M-256-16-baseline\"\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "# Quantization\n",
    "USE_8BIT = True\n",
    "USE_16BIT = False\n",
    "\n",
    "# Can only use 1 type of quantization\n",
    "assert not (USE_8BIT and USE_16BIT)\n",
    "\n",
    "# Prefix already set as \"test\"\n",
    "# DATASET_SUFFIX = \"_with_metadata\"\n",
    "save_location = \"results_baseline_8bit.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    \"\"\"Set seed for reproducibility.\"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "\n",
    "def collate_fn(examples, processor, to_bf16=False):\n",
    "    # Unpack the tuples into individual components\n",
    "    images = [example[0].convert(\"RGB\") for example in examples]\n",
    "    texts = [example[1].replace(\"\\n\", \"\") for example in examples]  # Prompts\n",
    "    labels = [example[2].replace(\"\\n\", \"\") for example in examples]  # Responses\n",
    "\n",
    "    # Process with the processor\n",
    "    tokens = processor(\n",
    "        text=texts,\n",
    "        images=images,\n",
    "        # suffix=labels, #! Make sure we don't supply the answer to the model\n",
    "        return_tensors=\"pt\",\n",
    "        padding=\"max_length\",\n",
    "        max_length=1024,  # hmm.\n",
    "        tokenize_newline_separately=False,\n",
    "    )\n",
    "\n",
    "    if to_bf16:\n",
    "        tokens = tokens.to(torch.bfloat16)\n",
    "\n",
    "    tokens[\"labels\"] = labels\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = ReVisionProcessor.from_pretrained(MODEL_ID, use_auth_token=HF_TOKEN)\n",
    "\n",
    "if USE_8BIT:\n",
    "    print(\"Using 8-bit quantization\")\n",
    "    config = BitsAndBytesConfig(\n",
    "        load_in_8bit=True, llm_int8_skip_modules=[\"vision_tower.vision_model\"]\n",
    "    )\n",
    "    model = ReVisionForConditionalGeneration.from_pretrained(\n",
    "        MODEL_ID,\n",
    "        use_auth_token=HF_TOKEN,\n",
    "        quantization_config=config,\n",
    "        device_map=DEVICE,\n",
    "    )\n",
    "    model = model.eval()\n",
    "\n",
    "elif USE_16BIT:\n",
    "    model = ReVisionForConditionalGeneration.from_pretrained(\n",
    "        MODEL_ID, use_auth_token=HF_TOKEN, torch_dtype=torch.float16, device_map=DEVICE\n",
    "    ).eval()\n",
    "else:\n",
    "    model = ReVisionForConditionalGeneration.from_pretrained(\n",
    "        MODEL_ID, use_auth_token=HF_TOKEN, device_map=DEVICE\n",
    "    ).eval()\n",
    "\n",
    "print(f\"Model is running on FPType: {model.dtype}\")\n",
    "\n",
    "# for name, param in model.named_parameters():\n",
    "#     print(f\"Layer: {name}, dtype: {param.dtype}\")\n",
    "\n",
    "# ! silences warning:\"Setting `pad_token_id` to `eos_token_id`:2 for open-end generation\"\n",
    "model.generation_config.pad_token_id = processor.tokenizer.pad_token_id\n",
    "\n",
    "# test_dataset = RevisionRewriteDatasetWithMetadata(\n",
    "#     split=\"test\",\n",
    "#     filename_suffix=DATASET_SUFFIX,\n",
    "#     use_auth_token=HF_TOKEN,\n",
    "#     processor=processor,\n",
    "# )\n",
    "test_dataset = RevisionRewriteDataset(split=\"test\",\n",
    "                                                  use_auth_token=HF_TOKEN,\n",
    "                                                  processor=processor)\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    collate_fn=lambda x: collate_fn(x, processor),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicted Completions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Completions From Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_predictions(test_dataloader, model, processor, device):\n",
    "    predictions, references = [], []\n",
    "    for batch in tqdm(test_dataloader):\n",
    "        batch_encoding = {k: v.to(device) for k, v in batch.items() if k != \"labels\"}\n",
    "        input_len = batch_encoding[\"input_ids\"].shape[-1]\n",
    "        with torch.no_grad():\n",
    "            output = model.generate(\n",
    "                **batch_encoding,\n",
    "                max_new_tokens=256,\n",
    "                do_sample=False,\n",
    "                repetition_penalty=1.5,\n",
    "            )\n",
    "        generated_texts = processor.batch_decode(\n",
    "            output[:, input_len:], skip_special_tokens=True\n",
    "        )\n",
    "\n",
    "        # remove \"assistant\\n\"\n",
    "        cleaned_texts = [text.removeprefix(\"assistant\\n\") for text in generated_texts]\n",
    "\n",
    "        predictions.extend(cleaned_texts)\n",
    "        references.extend(batch[\"labels\"])\n",
    "\n",
    "    return predictions, references\n",
    "\n",
    "\n",
    "def save_to_tsv(predictions, references, file_name=\"results.tsv\"):\n",
    "    # Create a DataFrame\n",
    "    data = {\"Prediction\": predictions, \"Reference\": references}\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Save DataFrame to a TSV file\n",
    "    df.to_csv(file_name, sep=\"\\t\", index=False)\n",
    "    print(f\"Results saved to: {file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions, references = generate_predictions(\n",
    "    test_dataloader, model, processor, DEVICE\n",
    ")\n",
    "save_to_tsv(predictions, references, save_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OR Load Completions From Local Storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you already obtained the predicted data, then you can run from this cell and below. Make sure you at least run the import section first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions_references_from_tsv(file_name: str):\n",
    "    df = pd.read_csv(file_name, delimiter=\"\\t\")\n",
    "    return df[\"Prediction\"].to_list(), df[\"Reference\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions, references = get_predictions_references_from_tsv(save_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_evaluations(predictions, references):\n",
    "    bleu_metric = evaluate.load(\"bleu\")\n",
    "    rouge_metric = evaluate.load(\"rouge\")\n",
    "    meteor_metric = evaluate.load(\"meteor\")\n",
    "\n",
    "    bleu_score = bleu_metric.compute(predictions=predictions, references=references)\n",
    "    rouge_score = rouge_metric.compute(predictions=predictions, references=references)\n",
    "    meteor_score = meteor_metric.compute(predictions=predictions, references=references)\n",
    "\n",
    "    print(\"BLEU Score:\", bleu_score)\n",
    "    print(\"ROUGE Score:\", rouge_score)\n",
    "    print(\"METEOR Score:\", meteor_score)\n",
    "\n",
    "    table = PrettyTable()\n",
    "    table.field_names = [\"Metric\", \"Score Details\"]\n",
    "    table.add_row([\"BLEU\", f\"{bleu_score['bleu']:.6f}\"])\n",
    "\n",
    "    rouge_details = (\n",
    "        f\"ROUGE-1: {rouge_score['rouge1']:.6f}, \"\n",
    "        f\"ROUGE-2: {rouge_score['rouge2']:.6f}, \"\n",
    "        f\"ROUGE-L: {rouge_score['rougeL']:.6f}\"\n",
    "    )\n",
    "    table.add_row([\"ROUGE\", rouge_details])\n",
    "    table.add_row([\"METEOR\", f\"{meteor_score['meteor']:.6f}\"])\n",
    "\n",
    "    print(\"Evaluation Metrics Report\")\n",
    "    print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_evaluations(predictions, references)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "revision",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
